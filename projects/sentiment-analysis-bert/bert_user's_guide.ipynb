{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsro5K6rwVHD",
        "outputId": "fedf6832-127c-4ef4-a7a4-071ca157ea34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Other-Tasks'...\n",
            "remote: Enumerating objects: 19, done.\u001b[K\n",
            "remote: Counting objects: 100% (19/19), done.\u001b[K\n",
            "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "remote: Total 19 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (19/19), 6.59 KiB | 6.59 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "# Clone the repo\n",
        "!git clone https://github.com/atikul-islam-sajib/Other-Tasks.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Change the directory\n",
        "%cd /content/Other-Tasks/projects/sentiment-analysis-bert/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVu6LZr2wYyT",
        "outputId": "79cbf62f-2e16-43ad-f471-2ffd78dc9121"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Other-Tasks/projects/sentiment-analysis-bert\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the requirements.txt\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3dDm4Apwj0o",
        "outputId": "68d53918-cb3f-487a-b708-16bded8088f4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (1.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (3.7.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (0.13.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (2.1.0+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (4.38.1)\n",
            "Collecting datasets (from -r requirements.txt (line 8))\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchinfo (from -r requirements.txt (line 11))\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Collecting xformers (from -r requirements.txt (line 12))\n",
            "  Downloading xformers-0.0.24-cp310-cp310-manylinux2014_x86_64.whl (218.2 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m218.2/218.2 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 1)) (2023.4)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 1)) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 2)) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 2)) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 2)) (3.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 4)) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 4)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 4)) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 4)) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 4)) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 4)) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 4)) (3.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 6)) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 6)) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 6)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 6)) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 6)) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 6)) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 6)) (2.1.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 7)) (0.20.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 7)) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 7)) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 7)) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 7)) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 7)) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 7)) (4.66.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 8)) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 8)) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets->-r requirements.txt (line 8))\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 8)) (3.4.1)\n",
            "Collecting multiprocess (from datasets->-r requirements.txt (line 8))\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 8)) (3.9.3)\n",
            "Collecting torch (from -r requirements.txt (line 6))\n",
            "  Downloading torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==2.2.0 (from torch->-r requirements.txt (line 6))\n",
            "  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (20.5 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 8)) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 8)) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 8)) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 8)) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 8)) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 8)) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->-r requirements.txt (line 1)) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 7)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 7)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 7)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 7)) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r requirements.txt (line 6)) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->-r requirements.txt (line 6)) (1.3.0)\n",
            "Installing collected packages: triton, torchinfo, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, torch, datasets, xformers\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.1.0\n",
            "    Uninstalling triton-2.1.0:\n",
            "      Successfully uninstalled triton-2.1.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.1.0+cu121\n",
            "    Uninstalling torch-2.1.0+cu121:\n",
            "      Successfully uninstalled torch-2.1.0+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.1.0+cu121 requires torch==2.1.0, but you have torch 2.2.0 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 2.2.0 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 2.2.0 which is incompatible.\n",
            "torchvision 0.16.0+cu121 requires torch==2.1.0, but you have torch 2.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.18.0 dill-0.3.8 multiprocess-0.70.16 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.3.101 nvidia-nvtx-cu12-12.1.105 torch-2.2.0 torchinfo-1.8.0 triton-2.2.0 xformers-0.0.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Install the dependencies\n",
        "!pip install accelerate -U"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzES4vRNyPu7",
        "outputId": "bdcaf0b4-6fd7-4d89-c282-8de6fc535b53"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting accelerate\n",
            "  Downloading accelerate-0.27.2-py3-none-any.whl (279 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.0)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.3.101)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.27.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make the changes in main1_train_BERT.py\n",
        "%cat src/main1_train_BERT.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "757-pwatw8am",
        "outputId": "518d3609-8b5b-4c5c-b183-37c6cbf170a4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "import os\r\n",
            "import numpy as np\r\n",
            "import pandas as pd\r\n",
            "# import matplotlib.pyplot as plt\r\n",
            "\r\n",
            "from datasets import load_dataset\r\n",
            "import numpy as np\r\n",
            "\r\n",
            "from transformers import AutoTokenizer, TrainingArguments, AutoModelForSequenceClassification, Trainer\r\n",
            "\r\n",
            "from pprint import pprint\r\n",
            "\r\n",
            "from datasets import load_metric\r\n",
            "\r\n",
            "from torchinfo import summary\r\n",
            "\r\n",
            "# -----------------------------------------------------------------------------\r\n",
            "\r\n",
            "# Get and print the current working directory\r\n",
            "current_working_directory = os.getcwd()\r\n",
            "print(f\"The current working directory is: {current_working_directory}\")\r\n",
            "\r\n",
            "# Change the working directory to a new directory (replace with the path you want)\r\n",
            "new_working_directory = \"E:\\Sentiment-Analyzer-BERT\"\r\n",
            "os.chdir(new_working_directory)\r\n",
            "\r\n",
            "# Get and print the new current working directory\r\n",
            "new_current_working_directory = os.getcwd()\r\n",
            "print(f\"The new current working directory is: {new_current_working_directory}\")\r\n",
            "\r\n",
            "# -----------------------------------------------------------------------------\r\n",
            "\r\n",
            "def tokenize_fn(batch):\r\n",
            "  return tokenizer(batch['sentence'], truncation=True)\r\n",
            "\r\n",
            "def compute_metrics(logits_and_labels):\r\n",
            "  # metric = load_metric(\"glue\", \"sst2\")\r\n",
            "  logits, labels = logits_and_labels\r\n",
            "  predictions = np.argmax(logits, axis=-1)\r\n",
            "  return metric.compute(predictions=predictions, references=labels)\r\n",
            "\r\n",
            "# -----------------------------------------------------------------------------\r\n",
            "\r\n",
            "\r\n",
            "raw_datasets = load_dataset(\"glue\", \"sst2\")\r\n",
            "\r\n",
            "raw_datasets\r\n",
            "raw_datasets['train']\r\n",
            "dir(raw_datasets['train'])\r\n",
            "type(raw_datasets['train'])\r\n",
            "\r\n",
            "\r\n",
            "raw_datasets['train'].data\r\n",
            "raw_datasets['train'][0]\r\n",
            "raw_datasets['train'][50000:50003]\r\n",
            "\r\n",
            "raw_datasets['train'].features\r\n",
            "\r\n",
            "# -----------------------------------------------------------------------------\r\n",
            "\r\n",
            "# checkpoint = \"bert-base-uncased\"\r\n",
            "checkpoint = \"distilbert-base-uncased\"\r\n",
            "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\r\n",
            "\r\n",
            "tokenized_sentences = tokenizer(raw_datasets['train'][0:3]['sentence'])\r\n",
            "\r\n",
            "pprint(tokenized_sentences)\r\n",
            "\r\n",
            "\r\n",
            "tokenized_datasets = raw_datasets.map(tokenize_fn, batched=True)\r\n",
            "\r\n",
            "training_args = TrainingArguments(\r\n",
            "  'my_trainer',\r\n",
            "  evaluation_strategy='epoch',\r\n",
            "  save_strategy='epoch',\r\n",
            "  num_train_epochs=1,\r\n",
            ")\r\n",
            "\r\n",
            "\r\n",
            "model = AutoModelForSequenceClassification.from_pretrained(\r\n",
            "    checkpoint,\r\n",
            "    num_labels=2)\r\n",
            "\r\n",
            "\r\n",
            "type(model)\r\n",
            "model\r\n",
            "\r\n",
            "summary(model)\r\n",
            "\r\n",
            "# -----------------------------------------------------------------------------\r\n",
            "\r\n",
            "params_before = []\r\n",
            "for name, p in model.named_parameters():\r\n",
            "  params_before.append(p.detach().cpu().numpy())\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "metric = load_metric(\"glue\", \"sst2\")\r\n",
            "\r\n",
            "metric.compute(predictions=[1, 0, 1], references=[1, 0, 0])\r\n",
            "\r\n",
            "# -----------------------------------------------------------------------------\r\n",
            "# Train and save BERT model\r\n",
            "\r\n",
            "trainer = Trainer(\r\n",
            "    model,\r\n",
            "    training_args,\r\n",
            "    train_dataset=tokenized_datasets[\"train\"],\r\n",
            "    eval_dataset=tokenized_datasets[\"validation\"],\r\n",
            "    tokenizer=tokenizer,\r\n",
            "    compute_metrics=compute_metrics,\r\n",
            ")\r\n",
            "\r\n",
            "trainer.train()\r\n",
            "\r\n",
            "\r\n",
            "trainer.save_model('my_saved_model')\r\n",
            "\r\n",
            "\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Change the working directory to a new directory (replace with the path you want)\n",
        "# Make sure you comment out this from main1_train_BERT.py\n",
        "'''\n",
        "  # Change the working directory to a new directory (replace with the path you want)\n",
        "  new_working_directory = \"E:\\Sentiment-Analyzer-BERT\"\n",
        "  os.chdir(new_working_directory)\n",
        "'''"
      ],
      "metadata": {
        "id": "0NHVrxVOx4DX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To train the model\n",
        "!python /content/Other-Tasks/projects/sentiment-analysis-bert/src/main1_train_BERT.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OQCWKhDwpu1",
        "outputId": "9dc86774-74de-41be-f9d6-1622823e1ba4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-03-03 19:28:31.460103: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-03 19:28:31.460159: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-03 19:28:31.461618: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-03 19:28:33.110003: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "The current working directory is: /content/Other-Tasks/projects/sentiment-analysis-bert\n",
            "The new current working directory is: /content/Other-Tasks/projects/sentiment-analysis-bert\n",
            "{'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
            " 'input_ids': [[101, 5342, 2047, 3595, 8496, 2013, 1996, 18643, 3197, 102],\n",
            "               [101,\n",
            "                3397,\n",
            "                2053,\n",
            "                15966,\n",
            "                1010,\n",
            "                2069,\n",
            "                4450,\n",
            "                2098,\n",
            "                18201,\n",
            "                2015,\n",
            "                102],\n",
            "               [101,\n",
            "                2008,\n",
            "                7459,\n",
            "                2049,\n",
            "                3494,\n",
            "                1998,\n",
            "                10639,\n",
            "                2015,\n",
            "                2242,\n",
            "                2738,\n",
            "                3376,\n",
            "                2055,\n",
            "                2529,\n",
            "                3267,\n",
            "                102]]}\n",
            "Map: 100% 872/872 [00:00<00:00, 10125.59 examples/s]\n",
            "model.safetensors: 100% 268M/268M [00:01<00:00, 193MB/s]\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "================================================================================\n",
            "Layer (type:depth-idx)                                  Param #\n",
            "================================================================================\n",
            "DistilBertForSequenceClassification                     --\n",
            "â”œâ”€DistilBertModel: 1-1                                  --\n",
            "â”‚    â””â”€Embeddings: 2-1                                  --\n",
            "â”‚    â”‚    â””â”€Embedding: 3-1                              23,440,896\n",
            "â”‚    â”‚    â””â”€Embedding: 3-2                              393,216\n",
            "â”‚    â”‚    â””â”€LayerNorm: 3-3                              1,536\n",
            "â”‚    â”‚    â””â”€Dropout: 3-4                                --\n",
            "â”‚    â””â”€Transformer: 2-2                                 --\n",
            "â”‚    â”‚    â””â”€ModuleList: 3-5                             42,527,232\n",
            "â”œâ”€Linear: 1-2                                           590,592\n",
            "â”œâ”€Linear: 1-3                                           1,538\n",
            "â”œâ”€Dropout: 1-4                                          --\n",
            "================================================================================\n",
            "Total params: 66,955,010\n",
            "Trainable params: 66,955,010\n",
            "Non-trainable params: 0\n",
            "================================================================================\n",
            "/content/Other-Tasks/projects/sentiment-analysis-bert/src/main1_train_BERT.py:98: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(\"glue\", \"sst2\")\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Downloading builder script: 5.76kB [00:00, 16.1MB/s]       \n",
            "{'loss': 0.3927, 'grad_norm': 9.589826583862305, 'learning_rate': 4.7030526190759e-05, 'epoch': 0.06}\n",
            "{'loss': 0.3419, 'grad_norm': 13.487079620361328, 'learning_rate': 4.4061052381518e-05, 'epoch': 0.12}\n",
            "{'loss': 0.3227, 'grad_norm': 41.239559173583984, 'learning_rate': 4.109157857227699e-05, 'epoch': 0.18}\n",
            "{'loss': 0.3185, 'grad_norm': 9.621081352233887, 'learning_rate': 3.812210476303599e-05, 'epoch': 0.24}\n",
            "{'loss': 0.2972, 'grad_norm': 7.213361740112305, 'learning_rate': 3.515263095379499e-05, 'epoch': 0.3}\n",
            "{'loss': 0.2748, 'grad_norm': 0.30620771646499634, 'learning_rate': 3.218315714455399e-05, 'epoch': 0.36}\n",
            "{'loss': 0.2571, 'grad_norm': 0.20468483865261078, 'learning_rate': 2.9213683335312986e-05, 'epoch': 0.42}\n",
            "{'loss': 0.2511, 'grad_norm': 0.33829060196876526, 'learning_rate': 2.6244209526071984e-05, 'epoch': 0.48}\n",
            "{'loss': 0.2483, 'grad_norm': 11.204866409301758, 'learning_rate': 2.3274735716830978e-05, 'epoch': 0.53}\n",
            "{'loss': 0.2431, 'grad_norm': 2.890162944793701, 'learning_rate': 2.0305261907589976e-05, 'epoch': 0.59}\n",
            "{'loss': 0.2434, 'grad_norm': 0.32219937443733215, 'learning_rate': 1.7335788098348973e-05, 'epoch': 0.65}\n",
            "{'loss': 0.2332, 'grad_norm': 26.545461654663086, 'learning_rate': 1.4366314289107971e-05, 'epoch': 0.71}\n",
            "{'loss': 0.2134, 'grad_norm': 5.916351795196533, 'learning_rate': 1.1396840479866969e-05, 'epoch': 0.77}\n",
            "{'loss': 0.2253, 'grad_norm': 12.156867027282715, 'learning_rate': 8.427366670625965e-06, 'epoch': 0.83}\n",
            "{'loss': 0.2217, 'grad_norm': 0.7906959056854248, 'learning_rate': 5.457892861384962e-06, 'epoch': 0.89}\n",
            "{'loss': 0.1938, 'grad_norm': 17.140539169311523, 'learning_rate': 2.4884190521439603e-06, 'epoch': 0.95}\n",
            "100% 8418/8419 [07:12<00:00, 20.96it/s]\n",
            "  0% 0/109 [00:00<?, ?it/s]\u001b[A\n",
            " 10% 11/109 [00:00<00:00, 104.68it/s]\u001b[A\n",
            " 20% 22/109 [00:00<00:00, 97.34it/s] \u001b[A\n",
            " 29% 32/109 [00:00<00:00, 91.70it/s]\u001b[A\n",
            " 39% 42/109 [00:00<00:00, 89.13it/s]\u001b[A\n",
            " 47% 51/109 [00:00<00:00, 89.11it/s]\u001b[A\n",
            " 55% 60/109 [00:00<00:00, 87.77it/s]\u001b[A\n",
            " 64% 70/109 [00:00<00:00, 89.13it/s]\u001b[A\n",
            " 73% 80/109 [00:00<00:00, 89.87it/s]\u001b[A\n",
            " 83% 90/109 [00:00<00:00, 91.15it/s]\u001b[A\n",
            " 92% 100/109 [00:01<00:00, 89.32it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.3414711356163025, 'eval_accuracy': 0.9048165137614679, 'eval_runtime': 1.2397, 'eval_samples_per_second': 703.377, 'eval_steps_per_second': 87.922, 'epoch': 1.0}\n",
            "100% 8419/8419 [07:13<00:00, 20.96it/s]\n",
            "100% 109/109 [00:01<00:00, 89.42it/s]\u001b[A\n",
            "{'train_runtime': 441.4002, 'train_samples_per_second': 152.58, 'train_steps_per_second': 19.073, 'train_loss': 0.26380542157480474, 'epoch': 1.0}\n",
            "100% 8419/8419 [07:21<00:00, 19.07it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the main2_play_trained_model.py\n",
        "%cat /src/main2_play_trained_model.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aG5sBKnVy4Og",
        "outputId": "33dbb9c4-03fd-4955-a6bb-c2813270dccd"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "import os\r\n",
            "import numpy as np\r\n",
            "import pandas as pd\r\n",
            "# import matplotlib.pyplot as plt\r\n",
            "\r\n",
            "# Get and print the current working directory\r\n",
            "current_working_directory = os.getcwd()\r\n",
            "print(f\"The current working directory is: {current_working_directory}\")\r\n",
            "\r\n",
            "# Change the working directory to a new directory (replace with the path you want)\r\n",
            "# new_working_directory = \"E:\\Sentiment-Analyzer-BERT\"\r\n",
            "# os.chdir(new_working_directory)\r\n",
            "\r\n",
            "# Get and print the new current working directory\r\n",
            "new_current_working_directory = os.getcwd()\r\n",
            "print(f\"The new current working directory is: {new_current_working_directory}\")\r\n",
            "\r\n",
            "\r\n",
            "#  ----------------------------------------------------------------------------\r\n",
            "from transformers import pipeline\r\n",
            "# from keras.saving.hdf5_format import save_attributes_to_hdf5_group\r\n",
            "\r\n",
            "\r\n",
            "# load trained, serialized model\r\n",
            "newmodel = pipeline('text-classification', model='my_saved_model') #, device=0)\r\n",
            "\r\n",
            "# Sentiment: positive\r\n",
            "print(newmodel('This movie is great!'))\r\n",
            "\r\n",
            "# Sentiment: negative\r\n",
            "print(newmodel('This movie sucks'))\r\n",
            "\r\n",
            "# Sentiment: positive\r\n",
            "print(newmodel('This movie is not bad'))\r\n",
            "\r\n",
            "# Sentiment: positive\r\n",
            "print(newmodel('This movie is not that bad'))\r\n",
            "\r\n",
            "# Sentiment: positive\r\n",
            "print(newmodel('I want to see it again'))\r\n",
            "\r\n",
            "# Sentiment: negative\r\n",
            "print(newmodel('I don\\'t want to see it again'))\r\n",
            "\r\n",
            "\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure you comment out this from main2_play_trained_model.py\n",
        "'''\n",
        "  # Change the working directory to a new directory (replace with the path you want)\n",
        "  new_working_directory = \"E:\\Sentiment-Analyzer-BERT\"\n",
        "  os.chdir(new_working_directory)\n",
        "'''"
      ],
      "metadata": {
        "id": "JeUJJv6HyDs5"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To test the model\n",
        "!python /src/main2_play_trained_model.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWJQNM1JyuoC",
        "outputId": "2838af48-cf1f-4824-cda4-cb7420fcc4be"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The current working directory is: /content/Other-Tasks/projects/sentiment-analysis-bert\n",
            "The new current working directory is: /content/Other-Tasks/projects/sentiment-analysis-bert\n",
            "2024-03-03 19:37:52.012427: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-03 19:37:52.012482: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-03 19:37:52.013932: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-03 19:37:53.109448: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[{'label': 'LABEL_1', 'score': 0.9996901750564575}]\n",
            "[{'label': 'LABEL_0', 'score': 0.9984697699546814}]\n",
            "[{'label': 'LABEL_1', 'score': 0.99709153175354}]\n",
            "[{'label': 'LABEL_1', 'score': 0.9936901330947876}]\n",
            "[{'label': 'LABEL_1', 'score': 0.9971662163734436}]\n",
            "[{'label': 'LABEL_0', 'score': 0.9655874967575073}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RDRdVHbN0ok5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}